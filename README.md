# Vision Language Models Playground

A sandbox for experimenting with Vision Language Models (VLM).

## Projects

### OpenAI API-compliant Multimodal Inference Client
  * [C++ Implementation](OpenAI-completion-client/cpp/Readme.md)
  * [Python Implementation](OpenAI-completion-client/python/Readme.md)

### Google AI Examples
  * Gemini API and Vertex AI examples in the [`google/`](google/) folder
  * See [Google AI documentation](docs/google.md) for setup guides and resources

### Llama Multimodal
  * Llama multimodal utilities in the [`llama/`](llama/) folder
  * See [Llama documentation](docs/llama.md) for code references

## Documentation

- **[Benchmarks](docs/benchmarks.md)** - VLM evaluation benchmarks and leaderboards
- **[Courses & Tutorials](docs/courses.md)** - Online courses and learning resources
- **[API Services](docs/api-services.md)** - Vision multimodal API providers
- **[Finetuning](docs/finetuning.md)** - Resources for finetuning VLMs
- **[RAG](docs/rag.md)** - Multimodal RAG resources
- **[Inference](docs/inference.md)** - Inference frameworks and tools
- **[Cloud GPU](docs/cloud-gpu.md)** - GPU rental services
- **[Google AI](docs/google.md)** - Google-specific resources (Gemini, Vertex AI)
- **[Llama](docs/llama.md)** - Llama-specific resources
