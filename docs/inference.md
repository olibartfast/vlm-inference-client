# Inference Resources

## TensorRT-LLM
Python/C++ inference client for TensorRT-LLM:
* [Multimodal Documentation](https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/multimodal/README.md)
* [C++ Executor Documentation](https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/cpp/executor/README.md)

## OpenVINO
Visual language assistant using OpenVINO:
* [DeepSeek-VL2 Notebook](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/deepseek-vl2/deepseek-vl2.ipynb)
* [Gemma3 Notebook](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/gemma3/gemma3.ipynb)
* [Qwen2.5-VL Notebook](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/qwen2.5-vl/qwen2.5-vl.ipynb)
